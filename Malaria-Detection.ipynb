{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPejcWSQ1Hj+LzI4R70afEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akankshakusf/Project-Deep-Learning-CNN-Malaria-Detection/blob/master/Malaria-Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HzsDZnF5hNII"
      },
      "outputs": [],
      "source": [
        "#import ML packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from datetime import datetime\n",
        "import cv2  #computer vision\n",
        "from sklearn.metrics import confusion_matrix,roc_curve\n",
        "\n",
        "#import DL tensorflow packages\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow_datasets as tfds\n",
        "import albumentations as A\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Layer,BatchNormalization, Input,InputLayer,Conv2D, MaxPool2D, Flatten,Dense,Dropout\n",
        "from tensorflow.keras.layers import RandomFlip,RandomRotation,Resizing,Rescaling,Reshape  #for data augmentation\n",
        "from tensorflow.keras.regularizers import L2, L1\n",
        "from tensorflow.keras.callbacks import Callback,CSVLogger,EarlyStopping,LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy,FalseNegatives,FalsePositives,TruePositives,\\\n",
        "TrueNegatives,Precision,Recall,AUC\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "#import wandb packages\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbCallback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What will we be carrying out?\n",
        "- By this we will be able to conclude that we can mix up models:\n",
        "    *  First we built Sequential model\n",
        "    *  Then we built Functional API Model which we broke down in feature_extractor_model which extracts features  and lenet_model_func where we flatten our features for final compiling and training\n",
        "    * Then I built complete model LenetModel mixing up Functional API Model's feature_extractor\n",
        "    * Ather that I go on to create a Custom dense layer NeuralearnDense"
      ],
      "metadata": {
        "id": "0nr-n5Zeh4Tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb Install,Login and Intialization"
      ],
      "metadata": {
        "id": "yATv85gah8p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAiRQw0nh4vG",
        "outputId": "6a75c5f0-1aff-40f9-9a40-c41c8b5761d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVphRIeWh-2J",
        "outputId": "03df78a3-0acd-412a-b7e8-7276f3428be2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makushwaha2\u001b[0m (\u001b[33makankshakusf2\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"Malaria-Detection\",\n",
        "    entity=\"akankshakusf2\",\n",
        "    #name = \"confusionmatrix\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "TXPGzL4wh-z7",
        "outputId": "3d9c59b1-03df-4de0-ad76-1b8a2d0df232"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makushwaha2\u001b[0m (\u001b[33makankshakusf2\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250409_133000-tfje7ugy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/akankshakusf2/Malaria-Detection/runs/tfje7ugy' target=\"_blank\">efficient-violet-3</a></strong> to <a href='https://wandb.ai/akankshakusf2/Malaria-Detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/akankshakusf2/Malaria-Detection' target=\"_blank\">https://wandb.ai/akankshakusf2/Malaria-Detection</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/akankshakusf2/Malaria-Detection/runs/tfje7ugy' target=\"_blank\">https://wandb.ai/akankshakusf2/Malaria-Detection/runs/tfje7ugy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/akankshakusf2/Malaria-Detection/runs/tfje7ugy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fc007349650>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.integration.keras import WandbCallback"
      ],
      "metadata": {
        "id": "7ddhFPCSh-xT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.tensorboard.unpatch()"
      ],
      "metadata": {
        "id": "3W32xZ2Ch-ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I used this just to start tensor borad on Wandb\n",
        "import wandb\n",
        "wandb.tensorboard.patch(root_logdir=\"./logs\")\n",
        "wandb.init(project=\"Malaria-D\", entity=\"akushwaha2-university-of-south-florida\")"
      ],
      "metadata": {
        "id": "oN7488wDiXVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.run"
      ],
      "metadata": {
        "id": "zzpyh7KyiXNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Params Dictionary"
      ],
      "metadata": {
        "id": "tlRC1NR4ihSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.config={\n",
        "        \"LEARNING_RATE\":0.001,\n",
        "        \"N_EPOCHS\":2,\n",
        "        \"BATCH_SIZE\":128,\n",
        "        \"DROPOUT_RATE\":0.0,\n",
        "        \"IM_SIZE\":224,\n",
        "        \"REGULARIZATION_RATE\":0.0,\n",
        "        \"N_FILTERS\":6,\n",
        "        \"KERNEL_SIZE\":3,\n",
        "        \"N_STRIDES\":1,\n",
        "        \"POOL_SIZE\":2,\n",
        "        \"N_DENSE_1\":128,\n",
        "        \"N_DENSE_2\":32,\n",
        "      }\n",
        "\n",
        "CONFIGURATION= wandb.config"
      ],
      "metadata": {
        "id": "I5C7tf7PiggH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "_hUFmCWfiw-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Malaria dataset from TensorFlow Datasets (TFDS)\n",
        "# and shuffle files\n",
        "# Setting as_supervised=True returns the data as (image, label) pairs and not dict\n",
        "\n",
        "dataset, dataset_info = tfds.load(\"malaria\", with_info=True,as_supervised=True, shuffle_files=True)"
      ],
      "metadata": {
        "id": "u1bpWIflixeN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to split train data\n",
        "\n",
        "def split(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n",
        "    # Get dataset size\n",
        "    DATASET_SIZE = tf.data.experimental.cardinality(dataset).numpy()\n",
        "\n",
        "    # Make train, val, test split\n",
        "    train_dataset = dataset.take(int(TRAIN_RATIO * DATASET_SIZE))\n",
        "    val_test_dataset = dataset.skip(int(TRAIN_RATIO * DATASET_SIZE))  # Skip train data\n",
        "    val_dataset = val_test_dataset.take(int(VAL_RATIO * DATASET_SIZE))\n",
        "    test_dataset = val_test_dataset.skip(int(VAL_RATIO * DATASET_SIZE))  # skip Remaining data as test\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "rOUOK0mni1i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### split function logic explaination builder ######\n",
        "\n",
        "# TRAIN_RATIO=0.6\n",
        "# VAL_RATIO=0.2\n",
        "# TEST_RATIO=0.2\n",
        "# TOTAL=10\n",
        "# #set range on data for testing logic\n",
        "# ds=tf.data.Dataset.range(TOTAL)\n",
        "# #train_sd,val_ds,test_ds=split(ds,TRAIN_RATIO,VAL_RATIO,TEST_RATIO)\n",
        "# train_ds=ds.take(int(TRAIN_RATIO*TOTAL))\n",
        "# val_test_ds=ds.skip(int(TRAIN_RATIO*TOTAL)) #not important\n",
        "# val_ds=val_test_ds.take(int(VAL_RATIO*TOTAL))\n",
        "# test_ds=val_test_ds.skip(int(VAL_RATIO*TOTAL))\n",
        "\n",
        "# #print main dataset for review\n",
        "# print([int(x) for x in ds.as_numpy_iterator()])\n",
        "# print([int(x) for x in train_ds.as_numpy_iterator()])\n",
        "# print([int(x) for x in val_test_ds.as_numpy_iterator()])\n",
        "# print([int(x) for x in val_ds.as_numpy_iterator()])\n",
        "# print([int(x) for x in test_ds.as_numpy_iterator()])\n"
      ],
      "metadata": {
        "id": "1gNvp74Si-tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle dataset before splitting\n",
        "dataset = dataset[\"train\"].shuffle(buffer_size=10000, reshuffle_each_iteration=False)"
      ],
      "metadata": {
        "id": "R3vundEejFfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split ratios\n",
        "# split it into three parts:\n",
        "# - The first 80% of the data will be used for training.\n",
        "# - The next 10% (80%-90%) will be used for validation\n",
        "# - The last 10% (90%-100%) will be used for testing.\n",
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "\n",
        "# Call split function\n",
        "train_dataset, val_dataset, test_dataset = split(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO)"
      ],
      "metadata": {
        "id": "w4MmEQyyjHKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset sizes\n",
        "print(f\"Training size: {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
        "print(f\"Validation size: {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
        "print(f\"Test size: {tf.data.experimental.cardinality(test_dataset).numpy()}\")"
      ],
      "metadata": {
        "id": "bzvRGsIXjLz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check data info\n",
        "dataset_info"
      ],
      "metadata": {
        "id": "5koTzw3djLpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- So, in the data 0 represent parasitic\n",
        "and 1 represents uneffected"
      ],
      "metadata": {
        "id": "H8D4IvVHjTWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check to values in dataset_info\n",
        "print(dataset_info.features['label'].int2str(0))\n",
        "print(dataset_info.features['label'].int2str(1))"
      ],
      "metadata": {
        "id": "-A0JtDj8jLmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few values from the validation dataset\n",
        "for i in val_dataset.take(1):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "Y917PvXgjLiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Visualization"
      ],
      "metadata": {
        "id": "R6R4ve5qjdhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (image, label) in enumerate(train_dataset.take(16)):  #Unpack tuple\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(image.numpy())  # Convert Tensor to NumPy array\n",
        "    plt.title(dataset_info.features['label'].int2str(label.numpy()))  # Convert label to class name\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "suToRoN7jLfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation (Only Resizing technique)"
      ],
      "metadata": {
        "id": "j2AiX1ptjiTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- These images are of bigger sizes (255,255,3) etc. But we will have to Normalize them and bring them in the range of 0-1 so Deep learningn model converges or inference faster"
      ],
      "metadata": {
        "id": "EuHUbMP1jkyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the image size we want to reduce to\n",
        "IM_SIZE=224\n",
        "BATCH_SIZE=32\n",
        "\n",
        "def resize_rescale(image, label):\n",
        "    # Resize and rescale the image\n",
        "    image = tf.image.resize(image, (IM_SIZE, IM_SIZE)) / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Apply the function to the dataset\n",
        "train_dataset = train_dataset.map(resize_rescale).shuffle(buffer_size=1000, reshuffle_each_iteration=True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.map(resize_rescale).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(resize_rescale).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "A8ZEz8t2jLcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "uhQSU35BjLZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###----- This is for testing batches as they give problems later in tensor to array conversion-----###\n",
        "\n",
        "# Count the total number of images in the train, val, and test datasets\n",
        "train_images = sum(1 for _ in train_dataset.unbatch())\n",
        "val_images = sum(1 for _ in val_dataset.unbatch())\n",
        "test_images = sum(1 for _ in test_dataset.unbatch())\n",
        "\n",
        "print(f\"Total images in train dataset: {train_images}\")\n",
        "print(f\"Total images in validation dataset: {val_images}\")\n",
        "print(f\"Total images in test dataset: {test_images}\")\n",
        "\n",
        "print(\"----------------------------------------------------------\")\n",
        "# Calculate the number of batches\n",
        "train_batches = train_images // BATCH_SIZE\n",
        "val_batches = val_images // BATCH_SIZE\n",
        "test_batches = test_images // BATCH_SIZE\n",
        "\n",
        "print(f\"Number of batches in train dataset: {train_batches}\")\n",
        "print(f\"Number of batches in validation dataset: {val_batches}\")\n",
        "print(f\"Number of batches in test dataset: {test_batches}\")  # after 2 full batches of 32, 3rd batch has only 23 images left."
      ],
      "metadata": {
        "id": "dIg_siMxjLWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Uncomment and run this to check batch that fail to meet shape criteria"
      ],
      "metadata": {
        "id": "ejXLmf1Ejq2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_last_batch_info(total_images, batch_size):\n",
        "#     # Calculate number of full batches\n",
        "#     full_batches = total_images // batch_size\n",
        "\n",
        "#     # Calculate the number of images in the last batch\n",
        "#     last_batch_size = total_images % batch_size\n",
        "\n",
        "#     # If there are no remainder images, last batch size will be the same as batch size\n",
        "#     if last_batch_size == 0:\n",
        "#         last_batch_size = batch_size  # The last batch will be of full size\n",
        "\n",
        "#     # Calculate the total number of batches\n",
        "#     total_batches = full_batches if last_batch_size == batch_size else full_batches + 1\n",
        "\n",
        "#     # Print the results\n",
        "#     print(f\"Total images: {total_images}\")\n",
        "#     print(f\"Batch size: {batch_size}\")\n",
        "#     print(f\"Number of full batches: {full_batches}\")\n",
        "#     print(f\"Total number of batches: {total_batches}\")\n",
        "#     print(f\"Size of the last batch: {last_batch_size}\")\n",
        "\n",
        "#     return last_batch_size\n",
        "\n",
        "# get_last_batch_info(2757,32)"
      ],
      "metadata": {
        "id": "6tCci0zGjLTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#view for verification\n",
        "for image, label in train_dataset.take(1):\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    print(\"Label:\", label)"
      ],
      "metadata": {
        "id": "83oRZncRjLPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#view for verification\n",
        "for image, label in test_dataset.take(1):\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    print(\"Label:\", label)"
      ],
      "metadata": {
        "id": "Df5hNWm2jLMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* see that image size is now 224 : of shape shape=(224, 224, 3), dtype=float32) tf.Tensor(1, shape=(), dtype=int64)\n",
        "- tf.Tensor(1, meaning its was not infected cell"
      ],
      "metadata": {
        "id": "IAaJ7vD0jwHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check for unique values in transformed train_dataset\n",
        "np.set_printoptions(suppress=True, precision=6)\n",
        "np.unique(image)"
      ],
      "metadata": {
        "id": "NYD6EZRijLJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Notice - here  goal is achieved of having all values in between 0 to 1"
      ],
      "metadata": {
        "id": "-gIrkr1JjzCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing** (Advanced)"
      ],
      "metadata": {
        "id": "cpRZCNBUj007"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation (Advanced)"
      ],
      "metadata": {
        "id": "FiVJ9Imbj316"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Augment images with Custom resize and rescale function\n",
        "#define the image size we want to reduce to\n",
        "\n",
        "IM_SIZE=224\n",
        "\n",
        "@tf.function ### Makes the function run faster by turning it into a TensorFlow graph\n",
        "\n",
        "##create resize_rescale fucntion\n",
        "def resize_rescale(image, label):\n",
        "    # Resize and rescale the image\n",
        "    image = tf.image.resize(image, (IM_SIZE, IM_SIZE)) / 255.0\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "sbTltCXcjLGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Rather that making custom functionf like above for  resizing and rescaling we can also make use of the tf.keras.layers resize and reshape methods\n",
        "- Rescaling-https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling?hl=en\n",
        "- Resizing https://www.tensorflow.org/api_docs/python/tf/keras/layers/Resizing?hl=en"
      ],
      "metadata": {
        "id": "IXtWE7Ymj8fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IM_SIZE = 224  #*RunThis\n",
        "#BATCH_SIZE = 32\n",
        "\n",
        "## 1.Augmented layer with \"layers method\" for resizing and rescaling\n",
        "resize_rescale_layers = tf.keras.Sequential([\n",
        "    Resizing(IM_SIZE, IM_SIZE),\n",
        "    Rescaling(1.0 / 255),\n",
        "])"
      ],
      "metadata": {
        "id": "NJQwva2zjLDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Augmentation using tf.image method : https://www.notion.so/Tensorflow-1a54ba18200f81f58982c39b61ce9ec8?pvs=4#1bf4ba18200f80e19a19dfc16dd092f7"
      ],
      "metadata": {
        "id": "EVG5Cho2j_eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1.create augmenting function with custom \"resize_rescale function\"\n",
        "\n",
        "@tf.function ### Makes the function run faster by turning it into a TensorFlow graph\n",
        "def augment(image,label):\n",
        "\n",
        "  #call resize_rescale function above to get image and label\n",
        "  image,label=resize_rescale(image,label)\n",
        "\n",
        "  # I will be using rot90 and Draw samples from a uniform distribution.\n",
        "  # using from  https://www.tensorflow.org/api_docs/python/tf/keras/random/uniform\n",
        "  image =tf.image.rot90(image,k=tf.random.uniform(shape=[],minval=0,maxval=2,dtype=tf.int32))\n",
        "\n",
        "  #using from https://www.tensorflow.org/api_docs/python/tf/image/adjust_saturation\n",
        "  #image=tf.image.adjust_saturation(image, saturation_factor=0.3)\n",
        "\n",
        "  #using from https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_saturation\n",
        "  #image=tf.image.stateless_random_saturation(image, 0.3, 0.5)\n",
        "\n",
        "  #using from https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_flip_left_right\n",
        "  image=tf.image.stateless_random_flip_left_right(image)\n",
        "\n",
        "  return image,label"
      ],
      "metadata": {
        "id": "4YD3hADdjLAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Making a custom Augment layer that inherit from \"Layer\" class\n",
        "class RotNinety(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  @tf.function ### Makes the function run faster by turning it into a TensorFlow graph\n",
        "  def call(self,image):\n",
        "    return tf.image.rot90(image,k=tf.random.uniform(shape=[],minval=0,maxval=2,dtype=tf.int32))\n"
      ],
      "metadata": {
        "id": "AB1tU3eRjK72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Augmentation using tf.layers, tf.image method to create custom Augmentation layer in Sequential API"
      ],
      "metadata": {
        "id": "IRZcrRp6kEfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### tf.keras.layer augment                        #*RunThis\n",
        "augment_layers = tf.keras.Sequential([\n",
        "       RandomRotation(factor = (0.25, 0.2501),),\n",
        "       RandomFlip(mode='horizontal',),\n",
        "\n",
        "])\n",
        "\n",
        "@tf.function\n",
        "def augment_layer(image, label):\n",
        "    # Ensure image is in (H, W, C) format\n",
        "    image = tf.ensure_shape(image, (None, None, 3))  # Allow dynamic shape\n",
        "    image = resize_rescale_layers(image)\n",
        "    image = augment_layers(image)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "xAP_1HHCjK5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 3. Augmentation: Flip, Rotate, Contrast\n",
        "# augment_layer = tf.keras.Sequential([\n",
        "#     RotNinety(),  #using the \"layer\" , \"image\" mthod together. Best of both worlds\n",
        "#     ##RandomRoatation(factor=(0.25,0.2501))  #no longer needed as we are able to use \"image\" mthd now which is better\n",
        "#     RandomFlip(mode='horizontal'),\n",
        "# ])\n",
        "\n",
        "# # For training: resize, rescale, then augment\n",
        "\n",
        "# @tf.function ### Makes the function run faster by turning it into a TensorFlow graph\n",
        "# def augment_layers(image, label):\n",
        "#     image = resize_rescale_layer(image)\n",
        "#     image = augment_layer(image,),# training=True)\n",
        "#     return image, label\n"
      ],
      "metadata": {
        "id": "WjGJUi34jK10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* why this union of tf.keras.layers and tf.keras.image was important?????\n",
        "- because \"image\" method is very good at handling images and the \"layers\" method provides us with its commendable speedly processing"
      ],
      "metadata": {
        "id": "SZXipzQckJTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # For validation/test: only resize and rescale\n",
        "# def resize_rescale_layers(image, label):\n",
        "#     image = resize_rescale_layer(image)\n",
        "#     return image, label"
      ],
      "metadata": {
        "id": "XU5XzJxQjKzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in train_dataset.take(1):\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    print(\"Label:\", label)"
      ],
      "metadata": {
        "id": "S6_96Y-KjKw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32   #*RunThis\n",
        "\n",
        "# Training pipeline with augmentation\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
        "    .map(augment_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "WDEdob6FjKg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation pipeline (only resize & rescale)  #*RunThis\n",
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .map(lambda image, label: (resize_rescale_layers(image, training=False), label))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n"
      ],
      "metadata": {
        "id": "UjPmDF7ljKd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test pipeline (only resize & rescale) #*RunThis\n",
        "test_dataset = (\n",
        "    test_dataset\n",
        "    .map(lambda image, label: (resize_rescale_layers(image, training=False), label))\n",
        "    # .batch(BATCH_SIZE)\n",
        "    # .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "fK6vMDXFjKan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "Zc4TzkJUjKYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "YePeM8nRjKVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "-OH_9HQ8jKSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Batching with different Augmentation techniques"
      ],
      "metadata": {
        "id": "mYK0wG6NkXmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(original,augmented):\n",
        "  # original image is a batch of 32 images\n",
        "  # take the first image from the batch for visualization\n",
        "\n",
        "  original = original[0]\n",
        "  augmented = augmented[0]\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(original)\n",
        "\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(augmented)\n"
      ],
      "metadata": {
        "id": "Z9TQja8njKQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_image, label= next(iter(train_dataset))"
      ],
      "metadata": {
        "id": "DiEtWdsOjKMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* there are several methods used to Augment the data we will be using adjust_saturation\n",
        "-adjust_brightness(...): Adjust the brightness of RGB or Grayscale images.\n",
        "-adjust_saturation(...): Adjust saturation of RGB images.\n",
        "-central_crop(...): Crop the central region of the image(s).\n",
        "-crop_and_resize(...): Extracts crops from the input image tensor and resizes them.\n",
        "-flip_left_right(...): Flip an image horizontally (left to right).\n",
        "-etc\n"
      ],
      "metadata": {
        "id": "y3CDwOgvkb5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I will be using adjust_saturation\n",
        "augmented_image=tf.image.adjust_saturation(original_image, saturation_factor=0.3)"
      ],
      "metadata": {
        "id": "iTvTPKySjKJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call the visualize fucntion\n",
        "visualize(original_image,augmented_image)"
      ],
      "metadata": {
        "id": "9Futw0J3jKHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sequential model performed very poorly with the this. Reason being simple that if I go back and see my augmentation I have used \"tf.image.adjust_saturation(image, saturation_factor=0.3)\" with the implementation of this"
      ],
      "metadata": {
        "id": "pO3UKfpskhjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## This is a testing cell\n",
        "# def visualize(original, augmented, original_labels):\n",
        "\n",
        "#   original_image = original[0]\n",
        "#   augmented_image = augmented[0]\n",
        "#   original_label = original_labels[0].numpy()  # Get the label as a NumPy value\n",
        "\n",
        "#   plt.subplot(1, 2, 1)\n",
        "#   plt.imshow(original_image)\n",
        "#   plt.title(f\"Original - Label: {original_label}\")  # Display label in title\n",
        "\n",
        "#   plt.subplot(1, 2, 2)\n",
        "#   plt.imshow(augmented_image)\n",
        "#   plt.title(\"Augmented Image\")  # Title for augmented image\n",
        "\n",
        "\n",
        "# for original_images, original_labels in train_dataset.take(1):  # Take 1 batch from the dataset\n",
        "#     augmented_images, augmented_labels = augment(original_images, original_labels)  # Apply augmentations\n",
        "\n",
        "#     # Now you can call the visualize function with the images and labels\n",
        "#     visualize(original_images, augmented_images, original_labels)  # Include original_labels\n"
      ],
      "metadata": {
        "id": "hN4ekHw1jKE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "66fn2kxdjKCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixup Data Augmentation\n",
        "- Note : This technique does not always work for all datasets"
      ],
      "metadata": {
        "id": "_Q6p9vtpkmcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1= train_dataset.shuffle(buffer_size=4096,).map(resize_rescale)\n",
        "train_dataset_2= train_dataset.shuffle(buffer_size=4096,).map(resize_rescale)\n",
        "\n",
        "mixed_dataset=tf.data.Dataset.zip(train_dataset_1, train_dataset_2) # comment when using Albumentation\n",
        "# mixed_dataset = train_dataset_1.concatenate(train_dataset_2) # use only when using Albumematation"
      ],
      "metadata": {
        "id": "LJ4tY_XMjJ_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a method for create mixup of images\n",
        "def mixup(train_dataset_1, train_dataset_2):\n",
        "  (image_1,label_1),(image_2,label_2) = train_dataset_1, train_dataset_2\n",
        "\n",
        "  image_1=cv2.resize(cv2.imread('cat.jpg'), (IM_SIZE,IM_SIZE))\n",
        "  image_2=cv2.resize(cv2.imread('dog.jpg'), (IM_SIZE,IM_SIZE))\n",
        "\n",
        "  lamda=tfp.distributions.Beta(0.2,0.2)\n",
        "  lamda=lamda.sample(1)[0]\n",
        "\n",
        "  image= lamda*image_1 + (1-lamda)*image_2\n",
        "  label= lamda*tf.cast(label_1,dtype=tf.float32) + (1-lamda)*tf.cast(label_2,dtype=tf.float32)\n",
        "\n",
        "  return image,label"
      ],
      "metadata": {
        "id": "ytBuLOIQjJ8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #### This is an example show how mix up works uncomment when needed\n",
        "\n",
        "# IM_SIZE=224\n",
        "# image_1=cv2.resize(cv2.imread('cat.jpg'), (IM_SIZE,IM_SIZE)) # manually upload a cat image in colab instance to see in action\n",
        "# image_2=cv2.resize(cv2.imread('dog.jpg'), (IM_SIZE,IM_SIZE))  # manually upload a cat image in colab instance to see in action\n",
        "\n",
        "# label_1= 0\n",
        "# label_2= 1\n",
        "\n",
        "# lamda=tfp.distributions.Beta(0.4,0.4)\n",
        "# lamda=lamda.sample(1)[0]\n",
        "\n",
        "# print(image_1.shape,image_2.shape)\n",
        "\n",
        "# image= lamda*image_1 + (1-lamda)*image_2\n",
        "# label= lamda*label_1 + (1-lamda)*label_2\n",
        "# print(image.shape, label)\n",
        "\n",
        "# plt.imshow(image/255)"
      ],
      "metadata": {
        "id": "_SnNuF6KjJ24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# Training pipeline with augmentation\n",
        "train_dataset = (\n",
        "    mixed_dataset\n",
        "    .shuffle(buffer_size=8, reshuffle_each_iteration=True)\n",
        "    .map(mixup)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Validation pipeline (only resize & rescale)\n",
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .map(resize_rescale)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# No need to do anything to test_dataset"
      ],
      "metadata": {
        "id": "XjIjTiHXjJzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "W8NXgTnFjJwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "HC8Wm8DzjJtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CutMix Data Augmentation"
      ],
      "metadata": {
        "id": "wN2TGEcFkwbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def box(lamda):\n",
        "\n",
        "  # Randomly select a center point (x, y) for the patch within the image\n",
        "  r_x = tf.cast(tfp.distributions.Uniform(0, IM_SIZE).sample(1)[0], dtype=tf.int32)\n",
        "  r_y = tf.cast(tfp.distributions.Uniform(0, IM_SIZE).sample(1)[0], dtype=tf.int32)\n",
        "\n",
        "  # Calculate the width and height of the patch based on how much of the image we want to replace\n",
        "  r_w = tf.cast(IM_SIZE * tf.math.sqrt(1 - lamda), dtype=tf.int32)\n",
        "  r_h = tf.cast(IM_SIZE * tf.math.sqrt(1 - lamda), dtype=tf.int32)\n",
        "\n",
        "  # Adjust the top-left corner so the patch stays within image boundaries\n",
        "  r_x = tf.clip_by_value(r_x - r_w // 2, 0, IM_SIZE)\n",
        "  r_y = tf.clip_by_value(r_y - r_h // 2, 0, IM_SIZE)\n",
        "\n",
        "  #find the x,y bottom right\n",
        "  x_b_r= tf.clip_by_value(r_x + r_w // 2, 0, IM_SIZE)\n",
        "  y_b_r = tf.clip_by_value(r_y + r_h // 2, 0, IM_SIZE)\n",
        "\n",
        "  #final value of r_w,r_h\n",
        "  r_w = x_b_r - r_x\n",
        "  if (r_w == 0):\n",
        "    r_w = 1\n",
        "\n",
        "  r_h = y_b_r - r_y\n",
        "  if (r_h == 0):\n",
        "    r_h = 1\n",
        "\n",
        "  # Print the top-left corner (r_x, r_y) and size (r_w, r_h) of the patch to be cut and mixed\n",
        "  return r_y, r_x, r_h, r_w\n"
      ],
      "metadata": {
        "id": "gD_hEodajJqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a method to perform MixUp by combining two training images\n",
        "\n",
        "def cutmix(train_dataset_1, train_dataset_2):\n",
        "  # Draw a random value from a Beta distribution to decide how much to mix two images\n",
        "  lamda = tfp.distributions.Beta(0.2, 0.2)\n",
        "  lamda = lamda.sample(1)[0]\n",
        "\n",
        "  r_y,r_x, r_h, r_w = box(lamda)\n",
        "\n",
        "  # Unpack images and labels from both datasets\n",
        "  (image_1, label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n",
        "\n",
        "  # Cut a rectangular patch from image_2\n",
        "  crop_2 = tf.image.crop_to_bounding_box(image_2, r_y, r_x, r_h, r_w)\n",
        "  # Place the cropped patch onto a blank canvas (same size as image) at a specific location\n",
        "  pad_2 = tf.image.pad_to_bounding_box(crop_2, r_y, r_x, IM_SIZE, IM_SIZE)\n",
        "\n",
        "  # Do the same crop and pad for image_1 (to subtract the patch area later)\n",
        "  crop_1 = tf.image.crop_to_bounding_box(image_1, r_y, r_x, r_h, r_w)\n",
        "  pad_1 = tf.image.pad_to_bounding_box(crop_1,r_y, r_x, IM_SIZE, IM_SIZE)\n",
        "\n",
        "  # Replace the patch in image_1 with the patch from image_2\n",
        "  image = image_1 - pad_1 + pad_2\n",
        "\n",
        "  lamda= tf.cast(1-(r_w*r_h)/(IM_SIZE*IM_SIZE),dtype=tf.float32)\n",
        "  label = lamda*tf.cast(label_1,dtype=tf.float32) + (1-lamda)*tf.cast(label_2,dtype=tf.float32)\n",
        "\n",
        "  # Return the mixed image and the label (currently label_1 only)\n",
        "  return image, label\n"
      ],
      "metadata": {
        "id": "R5NeASs1jJnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #### This is an example show how cutmix up works uncomment when needed\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(16,6))\n",
        "# plt.subplot(1,7,1)\n",
        "# image_1=cv2.resize(cv2.imread('cat.jpg'),(IM_SIZE,IM_SIZE))\n",
        "# plt.imshow(image_1)\n",
        "# #\n",
        "# plt.subplot(1,7,2)\n",
        "# image_2=cv2.resize(cv2.imread('dog.jpg'),(IM_SIZE,IM_SIZE))\n",
        "# plt.imshow(image_2)\n",
        "\n",
        "# plt.subplot(1,7,3)\n",
        "# # image_3=cv2.resize(cv2.imread('dog.jpg'),(IM_SIZE,IM_SIZE))\n",
        "# crop=tf.image.crop_to_bounding_box(image_2, 70, 50, 100, 98)\n",
        "# plt.imshow(crop)\n",
        "\n",
        "# plt.subplot(1,7,4)\n",
        "# image_4=tf.image.pad_to_bounding_box(crop, 20, 100, IM_SIZE, IM_SIZE)\n",
        "# plt.imshow(image_4)\n",
        "\n",
        "# plt.subplot(1,7,6)\n",
        "# cat_crop=tf.image.crop_to_bounding_box(image_1, 70, 50, 100, 98)\n",
        "# plt.imshow(cat_crop)\n",
        "\n",
        "\n",
        "# plt.subplot(1,7,7)\n",
        "# image_5=tf.image.pad_to_bounding_box(cat_crop, 20, 100, IM_SIZE, IM_SIZE)\n",
        "# plt.imshow(image_1-image_5+ image_4)"
      ],
      "metadata": {
        "id": "HW6-DMr6jJkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# Training pipeline with augmentation\n",
        "train_dataset = (\n",
        "    mixed_dataset\n",
        "    .shuffle(buffer_size=8, reshuffle_each_iteration=True)\n",
        "    .map(cutmix)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Validation pipeline (only resize & rescale)\n",
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .map(resize_rescale)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# No need to do anything to test_dataset"
      ],
      "metadata": {
        "id": "DGZ_IGhDjJhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "FAStnNDTjJeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "IEfqGeo3jJba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing: lets visualize to see what is going on\n",
        "original_image, label = next(iter(train_dataset))\n",
        "print(label)\n",
        "plt.imshow((original_image[0].numpy() * 255).astype(\"uint8\"))\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bfufAjZKjJY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Albumentations"
      ],
      "metadata": {
        "id": "1Mhi7jX8k91N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
      ],
      "metadata": {
        "id": "xLrNfBFAjJVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IM_SIZE=224"
      ],
      "metadata": {
        "id": "f_M381UKjJSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate augments\n",
        "# we can apply as many augments we want and adjust the values accordingly\n",
        "# here I have chosen the augments and their arguments at random\n",
        "\n",
        "\n",
        "transforms = A.Compose([\n",
        "    A.Resize(IM_SIZE, IM_SIZE),\n",
        "    A.OneOf([A.HorizontalFlip(),\n",
        "              A.VerticalFlip(),], p=0.3),\n",
        "    A.RandomRotate90(),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2,\n",
        "                               contrast_limit=0.2,  # or set this to a float, not False\n",
        "                                p=0.5),\n",
        "   #A.CoarseDropout(num_holes_range=(1, 8), hole_height_range=(0.1, 0.2), hole_width_range=(0.1, 0.2), fill=0, fill_mask=False, p=0.5)\n",
        "])\n"
      ],
      "metadata": {
        "id": "VVgM3P9IjJPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# used from https://albumentations.ai/docs/examples/tensorflow-example/\n",
        "def aug_albument(image):\n",
        "  data = {\"image\": image}\n",
        "  augmented = transforms(**data) #fit the data we have initialised above\n",
        "  augmented_image = augmented[\"image\"]\n",
        "  augmented_image = tf.cast(augmented_image / 255.0, tf.float32)  # Normalize\n",
        "  return augmented_image\n"
      ],
      "metadata": {
        "id": "JyrZ6g7IjJMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # here the above python function is converted back to tensorflow\n",
        "def process_data(image, label):\n",
        "    aug_img = tf.numpy_function(func=aug_albument, inp=[image], Tout=tf.float32)\n",
        "    # Set the shape explicitly after augmentation to avoid potential shape issues\n",
        "    aug_img.set_shape([IM_SIZE, IM_SIZE, 3])\n",
        "    return aug_img, label\n"
      ],
      "metadata": {
        "id": "L3GN-JZcjJJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .map(process_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "1RizpcYxjJEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset# Now, try to display the image again\n",
        "im, _ = next(iter(train_dataset))\n",
        "plt.imshow(im[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0kFMzrDijJB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(1,32):\n",
        "  plt.subplot(8,4,i)\n",
        "  plt.imshow(im[i])"
      ],
      "metadata": {
        "id": "cH93EiZ0jI_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeating the dataset (x5)"
      ],
      "metadata": {
        "id": "2CNuCF3IlNCm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r12WOMiUjI8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "riTVmQRxjI6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86esnhlkjI3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XeoB--ijI0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building advanced Models with Functional API, Subclassing and Custom Layers  using Keras API**"
      ],
      "metadata": {
        "id": "z4LJ0d94lPnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic: Sequential API:LeNet Model"
      ],
      "metadata": {
        "id": "JRI74DKMlTcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This exact model summary is explained with full walk through in notion https://www.notion.so/CNN-Layering-Neuron-Count-1bc4ba18200f80ffa385ea743e4a30cd"
      ],
      "metadata": {
        "id": "mMN8EEeglVGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clear up session cache\n",
        "from tensorflow.keras import backend as K\n",
        "# Clear the previous session to reset layer count\n",
        "K.clear_session()"
      ],
      "metadata": {
        "id": "iQS351bCjIwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                        #*RunThis\n",
        "'''\n",
        "1. Instantiate the CNN model \"Simple Sequential Model\"\n",
        "'''\n",
        "\n",
        "IM_SIZE=CONFIGURATION['IM_SIZE']\n",
        "DROPOUT_RATE=CONFIGURATION['DROPOUT_RATE']\n",
        "REGULARIZATION_RATE=CONFIGURATION['REGULARIZATION_RATE']\n",
        "N_FILTERS=CONFIGURATION['N_FILTERS']\n",
        "KERNEL_SIZE=CONFIGURATION['KERNEL_SIZE']\n",
        "N_STRIDES=CONFIGURATION['N_STRIDES']\n",
        "POOL_SIZE=CONFIGURATION['POOL_SIZE']\n",
        "\n",
        "lenet_model = tf.keras.Sequential([\n",
        "\n",
        "    InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n",
        "    #InputLayer(input_shape=(None, None, 3)),  # Input: IM_SIZE x IM_SIZE RGB image\n",
        "\n",
        "    #resize_rescale_layers,# embedding resize and rescale into SequentialAPI\n",
        "    #augment_layer,    # embedding augment into SequentialAPI\n",
        "\n",
        "    # 1st conv layer (extracts basic patterns)\n",
        "    Conv2D(filters=N_FILTERS, kernel_size=KERNEL_SIZE, strides=N_STRIDES, padding=\"valid\", activation=\"relu\",\n",
        "           kernel_regularizer=L2(REGULARIZATION_RATE)),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=POOL_SIZE, strides=N_STRIDES),  # Downsamples feature maps\n",
        "    Dropout(rate=DROPOUT_RATE),   #add a dropout layer\n",
        "\n",
        "    # 2nd conv layer (extracts deeper features)\n",
        "    Conv2D(filters=N_FILTERS*2, kernel_size=KERNEL_SIZE, strides=N_STRIDES, padding=\"valid\", activation=\"relu\",\n",
        "           kernel_regularizer=L2(REGULARIZATION_RATE)),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=POOL_SIZE, strides=N_STRIDES),  # Downsampling again\n",
        "\n",
        "\n",
        "    Flatten(),  # Converts 2D feature maps into 1D array\n",
        "\n",
        "    Dense(100, activation=\"relu\",kernel_regularizer=L2(REGULARIZATION_RATE)),  # Fully connected layer\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate=DROPOUT_RATE),   #add a dropout layer\n",
        "\n",
        "    Dense(10, activation=\"relu\",kernel_regularizer=L2(REGULARIZATION_RATE)),   # Further processing\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(1, activation=\"sigmoid\"),     # Output layer (binary classification)\n",
        "\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "lenet_model.summary()\n"
      ],
      "metadata": {
        "id": "pIsbbNLNjItI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functional API :LeNet Model"
      ],
      "metadata": {
        "id": "0lGTEl-LoIPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clear up session cache\n",
        "from tensorflow.keras import backend as K\n",
        "# Clear the previous session to reset layer count\n",
        "K.clear_session()"
      ],
      "metadata": {
        "id": "0t5ElyqDjIqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.a.Feature Extractor Model \"NON Sequential\"\n",
        "'''\n",
        "#create input function to send image\n",
        "func_input=Input(shape=(IM_SIZE, IM_SIZE, 3),name=\"Input_Image\")\n",
        "\n",
        "# 1st conv layer (extracts basic patterns)\n",
        "x= Conv2D(filters=6, kernel_size=3, strides=1, padding=\"valid\", activation=\"relu\")(func_input)\n",
        "x= BatchNormalization()(x)\n",
        "x= MaxPool2D(pool_size=2, strides=2)(x)  # Downsamples feature maps\n",
        "\n",
        "# 2nd conv layer (extracts deeper features)\n",
        "x= Conv2D(filters=16, kernel_size=3, strides=1, padding=\"valid\", activation=\"relu\")(x)\n",
        "x= BatchNormalization()(x)\n",
        "output= MaxPool2D(pool_size=2, strides=2)(x)  # Downsampling again\n",
        "\n",
        "#create lenet model\n",
        "feature_extractor_model = Model(func_input,output, name=\"Feature_Extractor\")\n",
        "feature_extractor_model.summary()"
      ],
      "metadata": {
        "id": "0useyNLijInV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.b.Feature Extractor with \"Sequential Model\"\n",
        "'''\n",
        "feature_extractor_seq_model = tf.keras.Sequential([\n",
        "\n",
        "    InputLayer(shape=(IM_SIZE, IM_SIZE, 3)),  # Input: IM_SIZE x IM_SIZE RGB image\n",
        "\n",
        "    # 1st conv layer (extracts basic patterns)\n",
        "    Conv2D(filters=6, kernel_size=3, strides=1, padding=\"valid\", activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=2, strides=2),  # Downsamples feature maps\n",
        "\n",
        "    # 2nd conv layer (extracts deeper features)\n",
        "    Conv2D(filters=16, kernel_size=3, strides=1, padding=\"valid\", activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=2, strides=2),  # Downsampling again\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "feature_extractor_seq_model.summary()\n"
      ],
      "metadata": {
        "id": "trm9mbEtoMtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callable Model"
      ],
      "metadata": {
        "id": "tpGEolfYoPGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "2. Flatten and instantiate the CNN model with above Sequential or Non Sequential model\n",
        "'''\n",
        "#create input function to send image\n",
        "func_input=Input(shape=(IM_SIZE, IM_SIZE, 3),name=\"Input_Image\")\n",
        "\n",
        "# 1st conv layer (extracts basic patterns)\n",
        "x= feature_extractor_seq_model(func_input)\n",
        "\n",
        "x= Flatten()(x) # Converts 2D feature maps into 1D array\n",
        "\n",
        "x= Dense(100, activation=\"relu\")(x)  # Fully connected layer\n",
        "x= BatchNormalization()(x)\n",
        "\n",
        "x= Dense(10, activation=\"relu\")(x)   # Further processing\n",
        "x= BatchNormalization()(x)\n",
        "\n",
        "func_output= Dense(1, activation=\"sigmoid\")(x)     # Output layer (binary classification)\n",
        "\n",
        "#create lenet model\n",
        "lenet_model_func = Model(func_input,func_output, name=\"Lenet_Model\")\n",
        "lenet_model_func.summary()"
      ],
      "metadata": {
        "id": "YevEQJVtoPiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Subclassing:LeNet Model"
      ],
      "metadata": {
        "id": "U8l5EQ3toSmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. making a class for feature extraction which will have subclass that inherits from \"Layer\" class\n",
        "'''\n",
        "class FeatureExtractor(Layer):\n",
        "  #create init Method\n",
        "  def __init__(self, filters, kernel_size, strides, padding, activation, pool_size):\n",
        "    super(FeatureExtractor,self).__init__()\n",
        "    self.conv_1  = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, activation=activation) # 1st conv layer (extracts basic patterns)\n",
        "    self.batch_1 = BatchNormalization()\n",
        "    self.pool_1  = MaxPool2D(pool_size=pool_size, strides=2*strides) # Downsamples feature maps\n",
        "\n",
        "\n",
        "    self.conv_2  = Conv2D(filters=filters*2, kernel_size=kernel_size, strides=strides, padding=padding, activation=activation) # 2nd conv layer (extracts deeper features)\n",
        "    self.batch_2 = BatchNormalization()\n",
        "    self.pool_2  = MaxPool2D(pool_size=pool_size, strides=2*strides) # Downsampling again\n",
        "\n",
        "  #create Call Method\n",
        "  def call(self,x,training):\n",
        "\n",
        "    x=self.conv_1(x)\n",
        "    x = self.batch_1(x, training=training)\n",
        "    x=self.pool_1(x)\n",
        "\n",
        "    x=self.conv_2(x)\n",
        "    x = self.batch_2(x, training=training)\n",
        "    x=self.pool_2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "feature_sub_classed=FeatureExtractor(8, 3, 1, \"valid\", \"relu\", 2)"
      ],
      "metadata": {
        "id": "f5-t9A7PoTFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "2.Flatten and instantiate the CNN model with Subclassing above\n",
        "'''\n",
        "#create input function to send image\n",
        "func_input=Input(shape=(IM_SIZE, IM_SIZE, 3),name=\"Input_Image\")\n",
        "\n",
        "# 1st conv layer (extracts basic patterns)\n",
        "x= feature_sub_classed(func_input,training=True)\n",
        "\n",
        "x= Flatten()(x) # Converts 2D feature maps into 1D array\n",
        "\n",
        "x= Dense(100, activation=\"relu\")(x)  # Fully connected layer\n",
        "x= BatchNormalization()(x)\n",
        "\n",
        "x= Dense(10, activation=\"relu\")(x)   # Further processing\n",
        "x= BatchNormalization()(x)\n",
        "\n",
        "func_output= Dense(1, activation=\"sigmoid\")(x)     # Output layer (binary classification)\n",
        "\n",
        "#create lenet model\n",
        "lenet_model_func = Model(func_input,func_output, name=\"Lenet_Model\")\n",
        "lenet_model_func.summary()"
      ],
      "metadata": {
        "id": "hKgqyiAioWGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.making a new model with above FeatureExtractor method for the extracting patterns\n",
        "'''\n",
        "class LenetModel(Model):\n",
        "  #create init Method\n",
        "  def __init__(self):\n",
        "    super(LenetModel,self).__init__()\n",
        "\n",
        "    self.feature_extractor  = FeatureExtractor(8, 3, 1, \"valid\", \"relu\", 2)\n",
        "\n",
        "    self.flatten = Flatten() # Converts 2D feature maps into 1D array\n",
        "\n",
        "    self.dense_1=Dense(100, activation=\"relu\") # Fully connected layer\n",
        "    self.batch_1=BatchNormalization()\n",
        "\n",
        "    self.dense_2=Dense(10, activation=\"relu\") # Further processing\n",
        "    self.batch_2=BatchNormalization()\n",
        "\n",
        "    self.dense_3=Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "\n",
        "  #create Call Method\n",
        "  def call(self,x,training=False):\n",
        "\n",
        "    x=self.feature_extractor(x, training=training)\n",
        "    x=self.flatten(x)\n",
        "    x=self.dense_1(x)\n",
        "    x=self.batch_1(x,training=training)\n",
        "    x=self.dense_2(x)\n",
        "    x=self.batch_2(x, training=training)\n",
        "    x=self.dense_3(x)\n",
        "    return x\n",
        "\n",
        "#instantiate the model:\n",
        "lenet_sub_classed = LenetModel()\n",
        "lenet_sub_classed(tf.zeros([1,224,224,3]))\n",
        "lenet_sub_classed.summary()"
      ],
      "metadata": {
        "id": "At-QYcoroY8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dense Layers"
      ],
      "metadata": {
        "id": "Axo0xa8tobSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  I create built this  method from this page :https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing\n",
        "* I also used - https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "* refer random_initializer = https://www.tensorflow.org/api_docs/python/tf/keras/initializers/RandomNormal"
      ],
      "metadata": {
        "id": "YDV1WT5toeBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explaination of Dense Layer**\n",
        "\n",
        "Dense Layer in Simple Terms:A dense layer in a neural network works by multiplying the inputs (features) by weights\n",
        "and then adding a bias. This can be written as:\n",
        "---> = mx + c\n",
        "Where:\n",
        "    m = weights\n",
        "    x = input features\n",
        "    c = bias\n",
        "\n",
        "Shapes of Matrices:\n",
        "- Input matrix has a shape of (b, f):\n",
        "    - b is the batch size (number of examples),\n",
        "    - f is the number of features (input size).\n",
        "  \n",
        "- Weights matrix has a shape of (f, o):\n",
        "    - f matches the input features,\n",
        "    - o is the output size (number of neurons in the layer).\n",
        "    \n",
        "So, understand here the shape should match meaning columns for 1 matrix (b,f)\n",
        "   should have same shape as rows of 2nd matrix(w,b)\n",
        "\n",
        "TensorFlow's Role:\n",
        "TensorFlow ensures that the number of features in the input matches the number of weights,\n",
        "so the multiplication works. If they don’t match, TensorFlow automatically adjusts the shapes\n",
        "to prevent errors.\n",
        "\n",
        "Finally, the bias is added to the result, and that's how the dense layer produces an output.\n"
      ],
      "metadata": {
        "id": "wBaXl5rhoipK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Linear(keras.layers.Layer):\n",
        "#     def __init__(self, units=32, input_dim=32):\n",
        "#         super().__init__()\n",
        "#         self.w = self.add_weight(\n",
        "#             shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
        "#         )\n",
        "#         self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         return tf.matmul(inputs, self.w) + self.b"
      ],
      "metadata": {
        "id": "yB9bM3NCob1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. Making Custom Dense Layer\n",
        "'''\n",
        "\n",
        "class NeuralearnDense(Layer):\n",
        "  def __init__(self, output_units, activation):\n",
        "    super(NeuralearnDense,self).__init__()\n",
        "    self.output_units = output_units\n",
        "    self.activation = activation\n",
        "\n",
        "\n",
        "  def build(self,input_features_shape):\n",
        "    # w stands for weights\n",
        "    self.w = self.add_weight(shape=(input_features_shape[-1], self.output_units),initializer=\"random_normal\",trainable=True)\n",
        "    # b stands for biases\n",
        "    self.b = self.add_weight(shape=(self.output_units,),initializer=\"random_normal\",trainable=True)\n",
        "\n",
        "\n",
        "  def call(self, input_features):\n",
        "    pre_output= tf.matmul(input_features,self.w) + self.b # w stands for weights , b for biases\n",
        "\n",
        "    if (self.activation ==\"relu\"):\n",
        "      return tf.nn.relu(pre_output)\n",
        "\n",
        "    elif (self.activation==\"sigmoid\"):\n",
        "      return tf.math.sigmoid(pre_output)\n",
        "\n",
        "    else:\n",
        "      return pre_output"
      ],
      "metadata": {
        "id": "nmSAMJ6aolkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "2.using my NeuralearnDense layer for the extracting patterns\n",
        "'''\n",
        "IM_SIZE = 224\n",
        "# Instantiate the CNN model\n",
        "lenet_custom_model = tf.keras.Sequential([\n",
        "\n",
        "    InputLayer(shape=(IM_SIZE, IM_SIZE, 3)),  # Input: IM_SIZE x IM_SIZE RGB image\n",
        "\n",
        "    # 1st conv layer (extracts basic patterns)\n",
        "    Conv2D(filters=6, kernel_size=3, strides=1, padding=\"valid\", activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=2, strides=2),  # Downsamples feature maps\n",
        "\n",
        "    # 2nd conv layer (extracts deeper features)\n",
        "    Conv2D(filters=16, kernel_size=3, strides=1, padding=\"valid\", activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=2, strides=2),  # Downsampling again\n",
        "\n",
        "    Flatten(),  # Converts 2D feature maps into 1D array\n",
        "\n",
        "    NeuralearnDense(100, activation=\"relu\"),  # Fully connected layer\n",
        "    BatchNormalization(),\n",
        "\n",
        "    NeuralearnDense(10, activation=\"relu\"),   # Further processing\n",
        "    BatchNormalization(),\n",
        "\n",
        "    NeuralearnDense(1, activation=\"sigmoid\"),     # Output layer (binary classification)\n",
        "\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "lenet_custom_model.summary()"
      ],
      "metadata": {
        "id": "JQnL4fDXolhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks Types"
      ],
      "metadata": {
        "id": "TEU5jnr5orHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- i have referred this : https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\n",
        "- This too:https://github.com/keras-team/keras/blob/v3.3.3/keras/src/callbacks/callback.py#L106-L119\n"
      ],
      "metadata": {
        "id": "D1O2RsrhotW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Callback"
      ],
      "metadata": {
        "id": "QLpciuMkowMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom callback class to monitor the model's loss during training\n",
        "class LossCallback(Callback):\n",
        "\n",
        "  #call this method at the end of each training epoch\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    # Print the loss value\n",
        "    print(\"\\nFor the Epoch Number {} the model has a loss of {:.5f}\".format(epoch+1, logs[\"loss\"]))\n",
        "\n",
        "  #call this method at the end of each training epoch\n",
        "  def on_batch_end(self,batch,logs):\n",
        "    print(\"\\nFor the Batch Number {} the model has a loss of {:.5f}\".format(batch+1, logs[\"loss\"]))\n"
      ],
      "metadata": {
        "id": "MOuEw3cNolfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSVLogger"
      ],
      "metadata": {
        "id": "7AGvk_NSozdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_callback=CSVLogger(\n",
        "    filename='logs.csv', separator=',', append=False\n",
        ")"
      ],
      "metadata": {
        "id": "vSL1qMSeolcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EarlyStopping"
      ],
      "metadata": {
        "id": "pKD4s2pho2Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of EarlyStoppping Callback\n",
        "earlystopping_callback=tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "earlystopping_callback"
      ],
      "metadata": {
        "id": "5VFXQh9do2hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorBoard"
      ],
      "metadata": {
        "id": "vtDC1wCwo5Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creaing proper dir so files are saved in proper format\n",
        "CURRENT_TIME= datetime.now().strftime('%d%m%y-%h%m%s')\n",
        "LOG_DIR = './logs/'+ CURRENT_TIME\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)"
      ],
      "metadata": {
        "id": "dcv8wBYRo5dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRIC_DIR = './logs/' + CURRENT_TIME + '/metrics'\n",
        "train_writer=tf.summary.create_file_writer(METRIC_DIR)"
      ],
      "metadata": {
        "id": "570ADftyo8Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LOG_DIR)\n",
        "print(METRIC_DIR)"
      ],
      "metadata": {
        "id": "SqsP3gKRo8Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LearningRateScheduler\n",
        "* i referred this - https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler"
      ],
      "metadata": {
        "id": "f5DMRNUHo_xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function keeps the initial learning rate for the first ten epochs\n",
        "# and decreases it exponentially after that.\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 1:\n",
        "        learning_rate= lr\n",
        "    else:\n",
        "        learning_rate= lr *float(tf.math.exp(-0.1))\n",
        "        #learning_rate=learning_rate.numpy()\n",
        "\n",
        "    with train_writer.as_default():\n",
        "      tf.summary.scalar('Learning Rate', data=learning_rate, step=epoch)\n",
        "    return learning_rate\n",
        "\n",
        "#initialize the scheduler and call the function\n",
        "scheduler_callback=LearningRateScheduler(scheduler,verbose=1)"
      ],
      "metadata": {
        "id": "ZsO0_ga5o8Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ModelCheckpoint"
      ],
      "metadata": {
        "id": "H5LOORjNpDXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='checkpoints.keras',\n",
        "    monitor='val_loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='auto',\n",
        "    save_freq= 3 #epoch\n",
        ")"
      ],
      "metadata": {
        "id": "LjMDP7Hio8Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "_zydZRnDpGxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plateau_callback=tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.1,\n",
        "    patience=2,\n",
        "    verbose=0\n",
        ")"
      ],
      "metadata": {
        "id": "gi9-NjiMo7_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Metrics Building (w/ and w/o params)"
      ],
      "metadata": {
        "id": "XSJBzhBzpJ1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Metric Class"
      ],
      "metadata": {
        "id": "1itOJrROpL_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import binary_accuracy    #*RunThis\n",
        "\n",
        "class CustomAccuracy(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='Custom_Accuracy', FACTOR=1):\n",
        "        super(CustomAccuracy, self).__init__(name=name)\n",
        "        self.FACTOR = FACTOR\n",
        "        self.accuracy = self.add_weight(name='accuracy', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Ensure y_true is float32 for binary_accuracy\n",
        "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "        # Compute binary accuracy (element-wise)\n",
        "        output = binary_accuracy(y_true, y_pred) * self.FACTOR\n",
        "        # Avoid division by zero\n",
        "        total_elements = tf.cast(tf.size(output), dtype=tf.float32)\n",
        "        acc_value = tf.reduce_sum(output) / (total_elements + tf.keras.backend.epsilon())\n",
        "        self.accuracy.assign(acc_value)\n",
        "\n",
        "    def result(self):\n",
        "        return self.accuracy\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.accuracy.assign(0.0)"
      ],
      "metadata": {
        "id": "SEsgHR1ypKPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Metric Method (with parametres)"
      ],
      "metadata": {
        "id": "5rpv1TJmpPNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_accuracy(FACTOR):\n",
        "  def metric(y_true, y_pred):\n",
        "    return binary_accuracy(y_true, y_pred)* FACTOR\n",
        "  return metric"
      ],
      "metadata": {
        "id": "d45yRclYpQka"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Binary Cross-Entropy Loss\n",
        "def custom_bce(y_true, y_pred):\n",
        "    bce = BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred)"
      ],
      "metadata": {
        "id": "oNMTn46apQhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "lenet_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.01),\n",
        "    loss=custom_bce,\n",
        "    metrics=[custom_accuracy]\n",
        ")"
      ],
      "metadata": {
        "id": "y8dSlqgnpQe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Losses Building (w/ and w/o params)"
      ],
      "metadata": {
        "id": "uCL9tsnNpbmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Custom Loss Class : This class inherits for Keras loss class: https://www.tensorflow.org/api_docs/python/tf/keras/losses"
      ],
      "metadata": {
        "id": "tSdc4Yf3pfqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBCE(tf.keras.losses.Loss):\n",
        "  def __init__(self,FACTOR):\n",
        "    super(CustomBCE,self).__init__()  # inherits from CustomBCE class\n",
        "    self.FACTOR=FACTOR\n",
        "\n",
        "  def call(self,y_true,y_pred):\n",
        "    bce = BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred)* self.FACTOR\n"
      ],
      "metadata": {
        "id": "oc0ACNX_pQZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics=[TruePositives(name='tp'),FalsePositives(name='fp'),TrueNegatives(name='tn'),FalseNegatives(name='fn'),\n",
        "         BinaryAccuracy(name='accuracy'),Precision(name='precision'),Recall(name='recall'),AUC(name='auc')]"
      ],
      "metadata": {
        "id": "tDN0Hv6FpQWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.compile(optimizer=Adam(learning_rate=0.01),\n",
        "                    loss=CustomBCE(FACTOR),\n",
        "                    metrics=metrics)"
      ],
      "metadata": {
        "id": "ZZNK9KYOpQT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Loss Method (with parametres)"
      ],
      "metadata": {
        "id": "DuaQhaxqpqW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FACTOR=1\n",
        "def custom_bce(FACTOR):\n",
        "  def loss(y_true, y_pred):\n",
        "    bce = BinaryCrossentropy()\n",
        "    return bce(y_true, y_pred)*FACTOR\n",
        "  return loss"
      ],
      "metadata": {
        "id": "4S-BERbOpQRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics=[TruePositives(name='tp'),FalsePositives(name='fp'),TrueNegatives(name='tn'),FalseNegatives(name='fn'),\n",
        "         BinaryAccuracy(name='accuracy'),Precision(name='precision'),Recall(name='recall'),AUC(name='auc')]"
      ],
      "metadata": {
        "id": "dF_izZenpQOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.compile(optimizer=Adam(learning_rate=0.01),\n",
        "                    loss=custom_bce(FACTOR),\n",
        "                    metrics=metrics)"
      ],
      "metadata": {
        "id": "fkIxrgk7pQLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Loss Method (without parametres): Albumentation --- Training"
      ],
      "metadata": {
        "id": "hr3_fabRpvud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom function to calculate loss           #*RunThis\n",
        "def custom_bce(y_true, y_pred):\n",
        "  bce = BinaryCrossentropy()\n",
        "  return bce(y_true, y_pred)"
      ],
      "metadata": {
        "id": "lrFHzRLtpQHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics=[TruePositives(name='tp'),FalsePositives(name='fp'),TrueNegatives(name='tn'),FalseNegatives(name='fn'),  #*RunThis\n",
        "         BinaryAccuracy(name='accuracy'),Precision(name='precision'),Recall(name='recall'),AUC(name='auc')]"
      ],
      "metadata": {
        "id": "KHN7_mNvpPpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.compile(optimizer=Adam(learning_rate=0.01),   #*RunThis\n",
        "                    loss=custom_bce,\n",
        "                    metrics=metrics)"
      ],
      "metadata": {
        "id": "xO63OSRip0C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "history =lenet_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=5,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "BNqnkA9zp1cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboard: Custom CallBack Training WRITER /LOGS"
      ],
      "metadata": {
        "id": "Zwav0qGzp6yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want to pull previous logs out\n",
        "#!rm -rf ./logs/"
      ],
      "metadata": {
        "id": "54EMT0uhp4T-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creaing proper dir so files are saved in proper format             #*RunThis\n",
        "CURRENT_TIME= datetime.now().strftime('%d%m%y-%h%m%s')\n",
        "\n",
        "CUSTOM_TRAIN_DIR = './logs/' + CURRENT_TIME + '/custom/train'\n",
        "CUSTOM_VAL_DIR = './logs/' + CURRENT_TIME + '/custom/val'\n",
        "\n",
        "custom_train_writer=tf.summary.create_file_writer(CUSTOM_TRAIN_DIR)\n",
        "custom_val_writer=tf.summary.create_file_writer(CUSTOM_VAL_DIR)"
      ],
      "metadata": {
        "id": "JFKdOG0jp4RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset=test_dataset.batch(1)"
      ],
      "metadata": {
        "id": "35YDy78Pp4NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogImagesCallbackTensorBoard(Callback):\n",
        "    def __init__(self, log_dir=\"./logs\"):\n",
        "        super().__init__()\n",
        "        CURRENT_TIME = datetime.now().strftime('%d%m%y-%H%M%S')\n",
        "        IMAGE_DIR = f\"{log_dir}/{CURRENT_TIME}/images\"\n",
        "        self.image_writer = tf.summary.create_file_writer(IMAGE_DIR)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        labels = []\n",
        "        inp = []\n",
        "\n",
        "        for x, y in test_dataset.as_numpy_iterator():\n",
        "            labels.append(y)\n",
        "            inp.append(x)\n",
        "\n",
        "        labels = np.array([i[0] for i in labels])\n",
        "        predicted = lenet_model.predict(np.array(inp)[:, 0, ...])\n",
        "        threshold = 0.5\n",
        "\n",
        "        cm = confusion_matrix(labels, predicted > threshold)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        sns.heatmap(cm, annot=True)\n",
        "        plt.title('Confusion matrix - {}'.format(threshold))\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.axis('off')\n",
        "\n",
        "        import io\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        plt.close()  # important to free memory\n",
        "\n",
        "        image = tf.image.decode_png(buffer.getvalue(), channels=3)\n",
        "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)  # normalize\n",
        "        image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "        with self.image_writer.as_default():\n",
        "            tf.summary.image(\"Confusion Matrix\", image, step=epoch)\n"
      ],
      "metadata": {
        "id": "R-W7vgZip4KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming LogImagesCallback is a class             #*RunThis\n",
        "log_images_callback = LogImagesCallbackTensorBoard()  # Create an instance"
      ],
      "metadata": {
        "id": "m84qyyFBp4HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb : Custom CallBack Training Metrics Results"
      ],
      "metadata": {
        "id": "IXFlUfs9qFVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Wandb ConfusionMatrix"
      ],
      "metadata": {
        "id": "kPBs98v2qHdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogImagesCallbackWandB(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        labels = []\n",
        "        inp = []\n",
        "\n",
        "        # Collect all test data\n",
        "        for x, y in test_dataset.as_numpy_iterator():\n",
        "            labels.append(y)\n",
        "            inp.append(x)\n",
        "\n",
        "        #Convert to numpy arrays (no indexing needed)\n",
        "        labels = np.array(labels)\n",
        "        inp = np.array(inp)\n",
        "\n",
        "        #make predictions\n",
        "        predicted = lenet_model.predict(inp)\n",
        "\n",
        "        #checking for my confirmation\n",
        "        print(\"labels\", labels, labels.dtype)\n",
        "        print(\"predicted\", predicted, predicted.dtype)\n",
        "\n",
        "        # Vectorized thresholding\n",
        "        pred = (predicted[:, 0] >= 0.5).astype(int) # we only want 0 or 1\n",
        "\n",
        "        # Log confusion matrix to WandB\n",
        "        wandb.log({\n",
        "            \"Confusion Matrix\": wandb.plot.confusion_matrix(\n",
        "                probs=None,\n",
        "                y_true=labels,\n",
        "                preds=pred,\n",
        "                class_names=[\"Parasitized\", \"Uninfected\"]\n",
        "            )\n",
        "        })\n"
      ],
      "metadata": {
        "id": "G6XrgHhsp4Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ###### this is a testing code to see \"preds\" in action\n",
        "\n",
        "# pred = []\n",
        "# # Assuming 'lenet_model' and 'test_dataset' are defined\n",
        "# # and 'lenet_model' is a trained model\n",
        "\n",
        "\n",
        "# # Batch the test dataset\n",
        "# BATCH_SIZE = 32  # Set an appropriate batch size\n",
        "# test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# # Assuming 'test_dataset' yields (image, label) pairs\n",
        "# for image, label in test_dataset:\n",
        "#     predicted = lenet_model.predict(image)  # Get predictions for the batch\n",
        "\n",
        "#   # Process individual predictions within the batch\n",
        "#     for i in range(len(predicted)):\n",
        "\n",
        "#       if(predicted[i][0]< 0.5):\n",
        "#         pred.append([1,0])\n",
        "\n",
        "#       else:\n",
        "#         pred.append([0,1])\n",
        "\n",
        "# print(pred)"
      ],
      "metadata": {
        "id": "HhvHX59cp4Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Wandb ROC curve"
      ],
      "metadata": {
        "id": "eWKHW2cQqMPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogImagesCallbackWandBPlot(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        labels = []\n",
        "        inp = []\n",
        "\n",
        "        for x, y in test_dataset.as_numpy_iterator():\n",
        "            labels.append(y)\n",
        "            inp.append(x)\n",
        "\n",
        "        labels = np.array(labels)\n",
        "        inp = np.array(inp)\n",
        "\n",
        "        #make predictions\n",
        "        predicted = lenet_model.predict(inp)\n",
        "\n",
        "        #checking for my confirmation\n",
        "        print(\"labels\", labels.shape, labels.dtype)\n",
        "        print(\"predicted\", predicted.shape, predicted.dtype)\n",
        "\n",
        "        # For sigmoid output: convert to shape (n_samples, 2)\n",
        "        pred = np.stack([1 - predicted[:, 0], predicted[:, 0]], axis=1) #this stack was a requirement of wandb\n",
        "        print(\"pred shape:\", pred.shape)\n",
        "\n",
        "        wandb.log({ \"ROC Curve\": wandb.plot.roc_curve(labels,pred,[\"Parasitized\", \"Uninfected\"])})\n"
      ],
      "metadata": {
        "id": "e5wrvVCPp37M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* wandb Log Images : Looks like Confusion Matrix\n"
      ],
      "metadata": {
        "id": "yyIYTgyWqQI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogImagesCallbackWandB(Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        labels = []\n",
        "        inp = []\n",
        "\n",
        "        for x, y in test_dataset.as_numpy_iterator():\n",
        "            labels.append(y)         # y is already a scalar (int), no need to index\n",
        "            inp.append(x)\n",
        "\n",
        "        labels = np.array(labels)\n",
        "        predicted = lenet_model.predict(np.array(inp))\n",
        "        threshold = 0.5\n",
        "\n",
        "        cm = confusion_matrix(labels, predicted > threshold)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        sns.heatmap(cm, annot=True)\n",
        "        plt.title('Confusion matrix - {}'.format(threshold))\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.axis('off')\n",
        "\n",
        "        import io\n",
        "        buffer = io.BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        plt.close()  # important to free memory\n",
        "\n",
        "        image_array = tf.image.decode_png(buffer.getvalue(), channels=3)\n",
        "\n",
        "        images = wandb.Image(image_array, caption=\"Confusion Matrix for epoch:{}\".format(epoch))\n",
        "\n",
        "        wandb.log({\"Confusion Matrix\": images})"
      ],
      "metadata": {
        "id": "SKXIRWJIp34u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "z0Fc0daXqUz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IM_SIZE=224\n",
        "\n",
        "\n",
        "def model_tune(hparams):\n",
        "      lenet_model = tf.keras.Sequential([\n",
        "      InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n",
        "\n",
        "      # 1st conv layer (extracts basic patterns)\n",
        "      Conv2D(filters=6, kernel_size=3, strides=1, padding=\"valid\",\n",
        "            activation=\"relu\",kernel_regularizer=L2(hparams[HP_REGULARIZATION_RATE])),\n",
        "      BatchNormalization(),\n",
        "      MaxPool2D(pool_size=2, strides=2),  # Downsamples feature maps\n",
        "      Dropout(rate=hparams[HP_DROPOUT]),   #add a dropout layer\n",
        "\n",
        "      # 2nd conv layer (extracts deeper features)\n",
        "      Conv2D(filters=16, kernel_size=3, strides=1, padding=\"valid\",\n",
        "      activation=\"relu\",kernel_regularizer=L2(hparams[HP_REGULARIZATION_RATE])),\n",
        "      BatchNormalization(),\n",
        "      MaxPool2D(pool_size=2, strides=2),  # Downsampling again\n",
        "\n",
        "      Flatten(),  # Converts 2D feature maps into 1D array\n",
        "\n",
        "      Dense(hparams[HP_NUM_UNIT_1], activation=\"relu\",\n",
        "            kernel_regularizer=L2(hparams[HP_REGULARIZATION_RATE])),  # Fully connected layer\n",
        "      BatchNormalization(),\n",
        "      Dropout(rate=hparams[HP_DROPOUT]),   #add a dropout layer\n",
        "\n",
        "      Dense(hparams[HP_NUM_UNIT_2], activation=\"relu\",\n",
        "            kernel_regularizer=L2(hparams[HP_REGULARIZATION_RATE])),   # Further processing\n",
        "      BatchNormalization(),\n",
        "\n",
        "      Dense(1, activation=\"sigmoid\"),     # Output layer (binary classification)\n",
        "\n",
        "  ])\n",
        "\n",
        "      lenet_model.compile(\n",
        "      optimizer=Adam(learning_rate = hparams[HP_LEARNING_RATE]),\n",
        "      loss=BinaryCrossentropy(),\n",
        "      metrics=['accuracy']\n",
        "      )\n",
        "\n",
        "      #fit the model\n",
        "      lenet_model.fit(val_dataset,epochs=1)\n",
        "      _, accuracy =lenet_model.evaluate(val_dataset)\n",
        "      return accuracy"
      ],
      "metadata": {
        "id": "Y5gVN3P1p31m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HP_NUM_UNIT_1= hp.HParam('num_units_1', hp.Discrete([16,32,64,128]))\n",
        "HP_NUM_UNIT_2= hp.HParam('num_units_2', hp.Discrete([16,32,64,128]))\n",
        "HP_DROPOUT= hp.HParam('dropout', hp.Discrete([0.1,0.2,0.3]))\n",
        "HP_REGULARIZATION_RATE= hp.HParam('regularization_rate', hp.Discrete([0.001,0.01,0.1]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-4, 1e-3]))\n"
      ],
      "metadata": {
        "id": "Z8degcKYp3zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to perform grid search\n",
        "run_number= 0\n",
        "for num_units_1 in HP_NUM_UNIT_1.domain.values:\n",
        "  for num_units_2 in HP_NUM_UNIT_2.domain.values:\n",
        "    for dropout_rate in HP_DROPOUT.domain.values:\n",
        "      for regularization_rate in HP_REGULARIZATION_RATE.domain.values:\n",
        "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
        "\n",
        "          hparams={\n",
        "              HP_NUM_UNIT_1:num_units_1,\n",
        "              HP_NUM_UNIT_2:num_units_2,\n",
        "              HP_DROPOUT: dropout_rate,\n",
        "              HP_REGULARIZATION_RATE:regularization_rate,\n",
        "              HP_LEARNING_RATE: learning_rate,\n",
        "          }\n",
        "\n",
        "          file_writer =tf.summary.create_file_writer('logs/'+str(run_number))\n",
        "\n",
        "          with file_writer.as_default():\n",
        "              hp.hparams(hparams)\n",
        "              accuracy = model_tune(hparams)\n",
        "              tf.summary.scalar('accuracy', accuracy, step = 0)\n",
        "          # Corrected variable names in the print statement\n",
        "          print(\"For the run {}, hparams num_units_1:{}, num_units_2:{}, dropout:{}, regularization_rate:{}, learning_rate:{}\".format(run_number, hparams[HP_NUM_UNIT_1], hparams[HP_NUM_UNIT_2],\n",
        "                                                             hparams[HP_DROPOUT], hparams[HP_REGULARIZATION_RATE],\n",
        "                                                             hparams[HP_LEARNING_RATE]))\n",
        "\n",
        "\n",
        "          run_number+=1"
      ],
      "metadata": {
        "id": "3K-9vYkIp3wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Training Loops"
      ],
      "metadata": {
        "id": "xeXNKKhUqcGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Simple Custom Training Loop"
      ],
      "metadata": {
        "id": "W7YR0Jc4qeKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPTIMIZER=Adam(learning_rate=0.01)                          #*RunThis\n",
        "METRIC= BinaryAccuracy()\n",
        "METRIC_VAL=BinaryAccuracy()\n",
        "EPOCHS=3"
      ],
      "metadata": {
        "id": "AUJj-eQvp3s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):                                 #*RunThis\n",
        "    print(f\"Train starts for epoch number {epoch + 1}\")\n",
        "\n",
        "    # Training loop\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as recorder:\n",
        "            y_pred = lenet_model(x_batch, training=True)\n",
        "            loss = custom_bce(y_batch, y_pred)\n",
        "\n",
        "        gradients = recorder.gradient(loss, lenet_model.trainable_weights)\n",
        "        OPTIMIZER.apply_gradients(zip(gradients, lenet_model.trainable_weights))\n",
        "        METRIC.update_state(y_batch, y_pred)\n",
        "\n",
        "    print(\"Training Loss:\", loss.numpy())\n",
        "    print(\"Training Accuracy:\", METRIC.result().numpy())\n",
        "    METRIC.reset_state()\n",
        "\n",
        "    # Validation loop\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        y_pred_val = lenet_model(x_batch_val, training=False)\n",
        "        loss_val = custom_bce(y_batch_val, y_pred_val)\n",
        "        METRIC_VAL.update_state(y_batch_val, y_pred_val)\n",
        "\n",
        "    print(\"Validation Loss:\", loss_val.numpy())\n",
        "    print(\"Validation Accuracy:\", METRIC_VAL.result().numpy())\n",
        "    METRIC_VAL.reset_state()\n"
      ],
      "metadata": {
        "id": "spNxmh7qp3p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Run same in Graph Mode  : You will notice its super fast"
      ],
      "metadata": {
        "id": "XmfpR1P2qjR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPTIMIZER=Adam(learning_rate=0.01)      #*RunThis\n",
        "METRIC= BinaryAccuracy()\n",
        "METRIC_VAL=BinaryAccuracy()\n",
        "EPOCHS=CONFIGURATION['N_EPOCHS']"
      ],
      "metadata": {
        "id": "rHWriXAsp3nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creaing proper dir so files are saved in proper format             #*RunThis\n",
        "CURRENT_TIME= datetime.now().strftime('%d%m%y-%h%m%s')\n",
        "\n",
        "CUSTOM_TRAIN_DIR = './logs/' + CURRENT_TIME + '/custom/train'\n",
        "CUSTOM_VAL_DIR = './logs/' + CURRENT_TIME + '/custom/val'\n",
        "\n",
        "custom_train_writer=tf.summary.create_file_writer(CUSTOM_TRAIN_DIR)\n",
        "custom_val_writer=tf.summary.create_file_writer(CUSTOM_VAL_DIR)"
      ],
      "metadata": {
        "id": "pi1aAIg7p3kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "              #*RunThis\n",
        "@tf.function\n",
        "def training_block(x_batch, y_batch):\n",
        "    with tf.GradientTape() as recorder:\n",
        "        y_pred = lenet_model(x_batch, training=True)\n",
        "        loss = custom_bce(y_batch, y_pred)\n",
        "\n",
        "    partial_derivatives = recorder.gradient(loss, lenet_model.trainable_weights)\n",
        "    OPTIMIZER.apply_gradients(zip(partial_derivatives, lenet_model.trainable_weights))\n",
        "    METRIC.update_state(y_batch, y_pred)\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def val_block(x_batch_val,y_batch_val):\n",
        "  y_pred_val= lenet_model(x_batch_val,training=False)\n",
        "  loss_val=custom_bce(y_batch_val,y_pred_val)\n",
        "  METRIC_VAL.update_state(y_batch_val,y_pred_val)\n",
        "  return loss_val\n"
      ],
      "metadata": {
        "id": "nfhxP5tep3hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):         #*RunThis\n",
        "  print(\"Train starts for epoch number {}\".format(epoch+1))\n",
        "  for step, (x_batch,y_batch) in enumerate(train_dataset):\n",
        "    loss= training_block(x_batch,y_batch)\n",
        "\n",
        "  print(\"Training Loss is \",loss.numpy())\n",
        "  print(\"The Accuracy is \",METRIC.result().numpy())\n",
        "  METRIC.reset_state()\n",
        "\n",
        "  #Including Validation\n",
        "  for (x_batch_val,y_batch_val) in val_dataset:\n",
        "    loss_val=val_block(x_batch_val,y_batch_val)\n",
        "\n",
        "  print(\"Validation Loss\",loss_val.numpy())\n",
        "  print(\"The Accuracy is \",METRIC_VAL.result().numpy())\n",
        "  METRIC_VAL.reset_state()"
      ],
      "metadata": {
        "id": "THbREowrp3eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Run same as a Custom Train Model: neuralearn"
      ],
      "metadata": {
        "id": "nWfODSwBqthC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPTIMIZER=Adam(learning_rate=0.01)      #*RunThis\n",
        "METRIC= BinaryAccuracy()\n",
        "VAL_METRIC=BinaryAccuracy()\n",
        "EPOCHS= 3"
      ],
      "metadata": {
        "id": "rDXiLxCRp3a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#*RunThis\n",
        "\n",
        "def neuralearn(model,loss_function,METRIC, VAL_METRIC, OPTIMIZER, train_dataset,val_dataset,EPOCHS):\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(\"Train starts for epoch number {}\".format(epoch+1))\n",
        "    for step, (x_batch,y_batch) in enumerate(train_dataset):\n",
        "      loss= training_block(x_batch,y_batch)\n",
        "\n",
        "    print(\"Training Loss is \",loss.numpy())\n",
        "    print(\"The Accuracy is \",METRIC.result().numpy())\n",
        "\n",
        "    #train_writer for loss\n",
        "    with custom_train_writer.as_default():\n",
        "      tf.summary.scalar('Training Loss', data=loss, step=epoch)\n",
        "    #train_writer for accuracy\n",
        "    with custom_train_writer.as_default():\n",
        "      tf.summary.scalar('Training Accuracy', data=METRIC.result(), step=epoch)\n",
        "\n",
        "    METRIC.reset_state()\n",
        "\n",
        "    #Including Validation\n",
        "    for (x_batch_val,y_batch_val) in val_dataset:\n",
        "      loss_val=val_block(x_batch_val,y_batch_val)\n",
        "\n",
        "    print(\"Validation Loss\",loss_val.numpy())\n",
        "    print(\"The Accuracy is \",METRIC_VAL.result().numpy())\n",
        "\n",
        "    #train_writer for loss\n",
        "    with custom_val_writer.as_default():\n",
        "      tf.summary.scalar('Validation Loss', data=loss_val, step=epoch)\n",
        "    #train_writer for accuracy\n",
        "    with custom_val_writer.as_default():\n",
        "      tf.summary.scalar('Validation Accuracy', data=METRIC_VAL.result(), step=epoch)\n",
        "\n",
        "    METRIC_VAL.reset_state()\n",
        "  print(\"Training Complete!!!\")"
      ],
      "metadata": {
        "id": "v72wJ7G4p3Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuralearn(lenet_model, custom_bce, METRIC, VAL_METRIC, OPTIMIZER, train_dataset,val_dataset,EPOCHS)  #*RunThis"
      ],
      "metadata": {
        "id": "cApZu68sp3VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Compiling and Training"
      ],
      "metadata": {
        "id": "H0sE909pq1bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                     #*RunThis\n",
        "metrics=[TruePositives(name='tp'),FalsePositives(name='fp'),TrueNegatives(name='tn'),FalseNegatives(name='fn'),\n",
        "         BinaryAccuracy(name='accuracy'),Precision(name='precision'),Recall(name='recall'),AUC(name='auc')]\n",
        "\n",
        "FACTOR=1\n",
        "LABELS=['Parasitized','Uninfected']"
      ],
      "metadata": {
        "id": "hs6zPgwwp3SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model                   #*RunThis\n",
        "lenet_model.compile(\n",
        "    optimizer=Adam(learning_rate=CONFIGURATION['LEARNING_RATE']),\n",
        "    loss=BinaryCrossentropy(),  #Binary Crossentropy for binary classification\n",
        "    metrics=metrics,\n",
        "    #run_eagerly=False # use only for debugging\n",
        ")"
      ],
      "metadata": {
        "id": "lAO8i1Ykp3PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model  for normal model                       #*RunThis custome func in wandb confusion matrix\n",
        "history = lenet_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=2,\n",
        "    verbose=True,\n",
        "    callbacks=[LogImagesCallbackWandB()])"
      ],
      "metadata": {
        "id": "a3_KPlxKp3MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unbatch and re-batch into a single batch to extract all data                 #*RunThis\n",
        "val_images, val_labels = next(iter(val_dataset.unbatch().batch(len(val_dataset))))"
      ],
      "metadata": {
        "id": "XpfrHtTFp3JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### use this when want to file model in Wandb      #*RunThisfor\n",
        "# Patch the model to prevent W&B crash\n",
        "lenet_model.stateful = False\n",
        "\n",
        "history = lenet_model.fit(train_dataset,\n",
        "                          validation_data=(val_images, val_labels),\n",
        "                          epochs=CONFIGURATION['N_EPOCHS'],\n",
        "                          verbose=True,\n",
        "                          callbacks=[WandbCallback(validation_data=(val_images, val_labels),\n",
        "                                    labels=LABELS,\n",
        "                                    input_type='image',\n",
        "                                    save_graph=False,\n",
        "                                    save_model=False,\n",
        "                              )])"
      ],
      "metadata": {
        "id": "lzDChSX4p3Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stop wandb\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "XBlVAG3vp3DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations"
      ],
      "metadata": {
        "id": "RB8xe7GwrBJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* test how the model is inferencing so far"
      ],
      "metadata": {
        "id": "GGoyrPherDX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CHECK MODEL INFERENCING ####\n",
        "\n",
        "image =cv2.imread(\"cell.jpg\")\n",
        "print(image.shape)\n",
        "image=tf.expand_dims(image,axis=0)\n",
        "print(image.shape)\n",
        "\n",
        "\n",
        "#now pass this into the mdoel so the model can do inferencing\n",
        "lenet_model.predict(image)"
      ],
      "metadata": {
        "id": "QV75jrhHp3Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "ke9gYBbup29Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard --logdir='./logs'"
      ],
      "metadata": {
        "id": "i1HDjhdUp26G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the losses\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K3Ukfh60p23h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the accuracies\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train_accuracy', 'val_accuracy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H_PKNzOEp20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Confusion Matrix"
      ],
      "metadata": {
        "id": "iedWxfIlrMsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ATTENTION : As we know Number of batches in test dataset: 86, And the last batch has"
      ],
      "metadata": {
        "id": "QvZICQT6rO1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through the batched dataset\n",
        "for x, y in test_dataset.as_numpy_iterator():\n",
        "    inputs.append(x)  # Collect images\n",
        "    labels.append(y)  # Collect labels\n",
        "\n",
        "# Concatenate and Flatten the list of batches into a single array\n",
        "inputs = np.concatenate(inputs, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)"
      ],
      "metadata": {
        "id": "rcUaMfksp2wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "id": "T7E_PVT4p2uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "id": "o2z4gN5wp2rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make the predictions\n",
        "predicted=lenet_model.predict(inputs)\n",
        "print(predicted[:,0])"
      ],
      "metadata": {
        "id": "8AEjBBIDp2pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot confusion matrix\n",
        "threshold=0.6265\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "cm=confusion_matrix(labels,predicted>threshold)\n",
        "print(cm)\n",
        "\n",
        "sns.heatmap(cm,annot=True, fmt='g',cmap=\"crest\")\n",
        "plt.title(\"Confusion Matrix -{}\".format(threshold))\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n"
      ],
      "metadata": {
        "id": "eSrVgKXrp2jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* making Confusion Matrix work with Tensor Boards"
      ],
      "metadata": {
        "id": "KkyKoeQWrXX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate FPR, TPR, and threshold values for ROC curve\n",
        "fp, tp, threshold = roc_curve(labels, predicted)\n",
        "\n",
        "# Plot ROC curve with FPR on x-axis and TPR on y-axis\n",
        "plt.plot(fp, tp)\n",
        "\n",
        "# Label axes\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "\n",
        "# Show grid\n",
        "plt.grid()\n",
        "\n",
        "# Skip some thresholds for cleaner labels\n",
        "skip =30\n",
        "\n",
        "# Annotate ROC curve with threshold values\n",
        "for i in range(0, len(threshold), skip):\n",
        "    plt.text(fp[i], tp[i], threshold[i])\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vwwY6119p2g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation and Testing**"
      ],
      "metadata": {
        "id": "c2ZnWjCarbQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model\n",
        "lenet_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "TxyKcJ7Cp2eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The losses are less and accuracy is good"
      ],
      "metadata": {
        "id": "i23d9oo8rfN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing model\n",
        "def parasite_or_not(x):\n",
        "  if x>0.5:\n",
        "    return str(\"P\")\n",
        "  else:\n",
        "    return str(\"U\")"
      ],
      "metadata": {
        "id": "zmgJ134_p2bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- model predictions\n",
        "- test_dataset.take(1) retrieves one batch from the dataset, but it remains a Dataset object.\n",
        "- model.predict() expects a NumPy array or Tensor, so test_dataset.take(1) must be properly extracted first.\n",
        "\n",
        "- [0] extracts the first batch of predictions.\n",
        "- [0] extracts the first image's prediction score (since it's a binary classification problem, the output is a single probability value)."
      ],
      "metadata": {
        "id": "IX3UiBUUrjbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make predictions\n",
        "print(parasite_or_not(lenet_model.predict(test_dataset.take(1))[0][0]))"
      ],
      "metadata": {
        "id": "O4bCLLq_p2Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize the predictions\n",
        "for image.label in test_dataset.take(1):\n",
        "  for i in range(9):\n",
        "    ax=plt.subplot(3,3,i+1)\n",
        "    plt.imshow(image[i])\n",
        "    #first display what label model predicted and then show its corresponding image\n",
        "    plt.title(parasite_or_not(label.numpy()[i])+ \"-\"+ parasite_or_not(lenet_model.predict(image)[i][0]))\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "53BdNPDmrmmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Saving model"
      ],
      "metadata": {
        "id": "0tzHMhYsrov9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using .keras to save\n",
        "lenet_model.save(\"/content/lenet_malaria_detection.keras\")"
      ],
      "metadata": {
        "id": "pXqDnuM8rq-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "# Load the saved model\n",
        "lenet_loaded_model = load_model(\"lenet_malaria_detection.keras\")\n",
        "# Check the model architecture\n",
        "lenet_loaded_model.summary()\n",
        "\n",
        "# Check if weights are loaded\n",
        "print(\"Loaded Model Weights:\", len(lenet_loaded_model.weights))"
      ],
      "metadata": {
        "id": "zveMy1l0rrwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- IT WAS THIS BEFORE SAVING :\n",
        "87/87 ━━━━━━━━━━━━━━━━━━━━ 16s 41ms/step - accuracy: 0.9634 - loss: 0.1453\n",
        "[0.41822549700737, 0.9651795625686646"
      ],
      "metadata": {
        "id": "skacPeLlrvr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the performance of loaded model\n",
        "# it should be similar as before\n",
        "\n",
        "#evaluate model\n",
        "lenet_loaded_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "CamIEqeFrrtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving to Google Drive"
      ],
      "metadata": {
        "id": "7eMGCcbCr1ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DXUU_tghrrqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/lenet/ /content/drive/MyDrive/lenet_colab/"
      ],
      "metadata": {
        "id": "AkBslLW7rrm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/lenet_colab/ /content/lenet_colab/"
      ],
      "metadata": {
        "id": "Mh6oksGTrrjY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}